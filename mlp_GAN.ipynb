{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d32174e-0bb7-4617-b7b8-625f43274c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef039be-c706-4e3a-910d-76c332607ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa6f2e-18ab-41e7-8920-4db181be2d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6aff28-60a6-4a7c-afe8-69e9d1102f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebDataset(Dataset):\n",
    "    def __init__(self, image_dir, img_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.img_transform = img_transform\n",
    "        self.image_paths = sorted(os.listdir(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70dab07b-2239-4ac2-b8be-e18bba7be249",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = CelebDataset(\n",
    "    image_dir=\"img_align_celeba/img_align_celeba\",\n",
    "    img_transform=img_transform\n",
    ")\n",
    "\n",
    "test_dataset = CelebDataset(\n",
    "    image_dir=\"test\",\n",
    "    img_transform=img_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef046681-e377-4623-95b5-71c63813f0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202599"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4acf854-90ff-4e4c-8eb1-b963ae242824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e27388-d9c8-4459-8583-4000def04f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim=100, img_dim=6912):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_dim = img_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(500, img_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "   \n",
    "\n",
    "    def sample(self, batch_size=64, device=\"cpu\"):\n",
    "        z = torch.randn(batch_size, self.z_dim, device=device)\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.net(z)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a72369-a9fb-4de5-90f7-03a2eedc51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim=6912):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_dim = img_dim\n",
    "    \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(img_dim, 240),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(240, 240),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(240, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e1dd41-1414-4ec5-83c6-e7e472c4d628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b2756a-6f04-4386-a367-79a250fa3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1c4e87-b233-49bb-8405-2f333fb4072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bc8f34-64eb-4ec7-a3df-18a6ae25f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LD​=−(logD(x)+log(1−D(G(z))))  >>> this is the original discriminator loss from the GAN paper.\n",
    "# Now in PyTorch, instead of manually writing log() terms, people usually use nn.BCELoss() (binary cross entropy), which already implements\n",
    "# BCE(p,y)=−(y⋅log(p)+(1−y)⋅log(1−p))\n",
    "\n",
    "# For real images (y=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46641284-ff9e-4d19-a7c3-127e961f2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "real_images = next(iter(train_loader))\n",
    "print(real_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea58a1b-f0da-4324-b2ff-32f48290b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = next(iter(train_loader))\n",
    "real_images = real_images.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccbcf7e6-27bb-4966-89ba-a0753b6c4404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 48, 48])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d08ff2f-fb19-4cec-a0ab-6de711d49599",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_loss = -(1*torch.log(disc(real_images)) + (1-1)*torch.log(1-disc(real_images)+ 1e-8)).mean()  # −logD(x)\n",
    "# LD​=−(logD(x)+log(1−D(G(z)))) above is for y=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "696ede62-2c82-4a23-81eb-bd3653e313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For real images (y=0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a4b3da-2be6-4282-9718-5ba6645be3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = torch.randn(batch_size, latent_dim).to(device)\n",
    "z = gen.sample(batch_size=real_images.size(0), device=device)\n",
    "fake_images = gen(z)\n",
    "fake_images_reshaped = fake_images.view(-1, 3, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b86311d-cf90-4a24-8e4d-b1f7bd217126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6912])\n",
      "torch.Size([4, 3, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "print(fake_images.shape)\n",
    "print(fake_images_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55a1f117-7f07-40ce-9e78-d49b1aea9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_loss = -(0*torch.log(disc(fake_images_reshaped)) + (1-0)*torch.log(1-disc(fake_images_reshaped)) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba0a64e9-de4e-4dc1-8aab-3db578060465",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss = real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b40a333f-efea-4b77-bbc1-2ede8fad1e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b27522-51d5-4dc6-a360-970ed429c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the generator loss\n",
    "# LG​=−log(D(G(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc122bf1-83de-4ff6-9b7c-94ebed0a1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss = -(torch.log(disc(fake_images_reshaped)) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53a7a5d0-d977-408f-b231-0105f0afc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_D = torch.optim.Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_G = torch.optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9374ae8-237a-410c-971c-4df10abf380a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999fd787d1654170ae4cb2dc65509f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [1/3]:   0%|          | 0/50650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], D Loss: nan, G Loss: nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839a4b680efc4c84a166f498118afb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [2/3]:   0%|          | 0/50650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], D Loss: nan, G Loss: nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4253865bb1a2485e8925772827c2efd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [3/3]:   0%|          | 0/50650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], D Loss: nan, G Loss: nan\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_d = 0.0\n",
    "    epoch_loss_g = 0.0\n",
    "\n",
    "    # Wrap train_loader with tqdm\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=True)\n",
    "    \n",
    "    for real_images in loop:\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real images loss\n",
    "        real_loss = -(1*torch.log(disc(real_images)) + (1-1)*torch.log(1-disc(real_images)) + 1e-8).mean()  \n",
    "\n",
    "        z = gen.sample(batch_size=real_images.size(0), device=device)\n",
    "        fake_images = gen(z)\n",
    "        fake_images_reshaped = fake_images.view(-1, 3, 48, 48)\n",
    "        fake_loss = -(0*torch.log(disc(fake_images_reshaped.detach())) + (1-0)*torch.log(1-disc(fake_images_reshaped.detach())) + 1e-8).mean()\n",
    "\n",
    "        d_loss = real_loss + fake_loss\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        # Generator tries to fool discriminator\n",
    "        g_loss = -(torch.log(disc(fake_images_reshaped)) + 1e-8).mean()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        epoch_loss_d = epoch_loss_d + d_loss.item()\n",
    "        epoch_loss_g = epoch_loss_g + g_loss.item()\n",
    "\n",
    "        # Update tqdm description with losses\n",
    "        loop.set_postfix(D_loss=epoch_loss_d/(loop.n+1), G_loss=epoch_loss_g/(loop.n+1))\n",
    "\n",
    "    avg_loss_d = epoch_loss_d / len(train_loader)\n",
    "    avg_loss_g = epoch_loss_g / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {avg_loss_d:.4f}, G Loss: {avg_loss_g:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65fe3c6d-4980-4f2c-877f-bfcdfb7d7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), \"generator.pth\")\n",
    "torch.save(disc.state_dict(), \"discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b70cb3eb-3e2f-4308-93d1-e388790a37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"optimizer_G_state_dict\": optimizer_G.state_dict(),\n",
    "    \"optimizer_D_state_dict\": optimizer_D.state_dict(),\n",
    "    \"epoch\": epoch  # optional, for checkpointing\n",
    "}, \"gan_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7fd01e5-4b67-4070-a67d-68ee4110880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.load_state_dict(torch.load(\"generator.pth\"))\n",
    "disc.load_state_dict(torch.load(\"discriminator.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30e3e3d7-977e-4e0e-819a-ff9f498ece29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=500, out_features=6912, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim=100, img_dim=6912).to(device)\n",
    "gen.load_state_dict(torch.load(\"generator.pth\", map_location=device))\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2d185f9-2155-45e2-971f-16aa24b7f8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=6912, out_features=240, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=240, out_features=240, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=240, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55a01151-1c21-414a-b1b2-dcebe5c9cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = number of images you want to generate\n",
    "z = gen.sample(batch_size=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86701aae-ae12-445a-b3d4-c3ffda0fb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fake_image = gen(z)  # shape [1, 6912]\n",
    "\n",
    "# Reshape back to image format [batch, channels, height, width]\n",
    "fake_image = fake_image.view(1, 3, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72964690-945a-463d-a8e3-4ceb632581e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  npimg = (npimg * 255).astype(np.uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE90lEQVR4nO3bIQ7EMAwAweTU/3/ZxxaHRC2YwQZmKwPvmZkFAGut39sLAPAdogBARAGAiAIAEQUAIgoARBQAiCgAkOd0cO99cw8ALjv5VXYpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ53RwZm7uAcAHuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMgfGXENBwN97qoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unnormalize (back to [0,1] range)\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
    "\n",
    "fake_image = fake_image * std + mean\n",
    "fake_image = torch.clamp(fake_image, 0, 1)\n",
    "\n",
    "# Convert to CPU numpy for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "plt.imshow(T.ToPILImage()(fake_image.squeeze(0).cpu()))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e6b6521-7479-407f-acf4-a63ff9b1fc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE90lEQVR4nO3bIQ7EMAwAweTU/3/ZxxaHRC2YwQZmKwPvmZkFAGut39sLAPAdogBARAGAiAIAEQUAIgoARBQAiCgAkOd0cO99cw8ALjv5VXYpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ53RwZm7uAcAHuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMgfGXENBwN97qoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load trained generator\n",
    "gen = Generator()\n",
    "gen.load_state_dict(torch.load(\"generator.pth\", map_location=\"cpu\"))\n",
    "gen.eval()\n",
    "\n",
    "# Generate noise\n",
    "z = torch.randn(1, 100)  # batch=1, z_dim=100\n",
    "\n",
    "# Generate fake image\n",
    "with torch.no_grad():\n",
    "    fake_img = gen(z)\n",
    "\n",
    "# Reshape (assuming 48x48x3)\n",
    "fake_img = fake_img.view(48, 48, 3).cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.imshow(fake_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be966882-119f-40e6-9290-0688c205509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE90lEQVR4nO3bIQ7EMAwAweTU/3/ZxxaHRC2YwQZmKwPvmZkFAGut39sLAPAdogBARAGAiAIAEQUAIgoARBQAiCgAkOd0cO99cw8ALjv5VXYpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ53RwZm7uAcAHuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMgfGXENBwN97qoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_img = gen(z).cpu().view(48, 48, 3).detach().numpy()\n",
    "\n",
    "# Normalize for visualization\n",
    "fake_img = (fake_img - fake_img.min()) / (fake_img.max() - fake_img.min() + 1e-8)\n",
    "\n",
    "plt.imshow(fake_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c39e9-4f2c-4751-9179-7bce4706ecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
